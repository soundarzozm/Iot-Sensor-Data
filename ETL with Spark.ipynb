{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2>EXTRACT TRANSFORM LOAD NOTEBOOK</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The below code-block is to integrate the notebook with the appropriate *project token* so as to work with the project resources."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let's start by installing PySpark."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting pyspark==2.4.5\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 217.8MB 149kB/s eta 0:00:0169.6MB/s eta 0:00:13\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589         | 155.2MB 9.4MB/s eta 0:00:07\n\u001b[?25hCollecting py4j==0.10.7 (from pyspark==2.4.5)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 40.4MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.5\n"
                }
            ],
            "source": "#Install PySpark\n!pip install pyspark==2.4.5"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now we can import the necessary libraries for ETL."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Import necessary libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Apache Spark requires a session initialization before reading the data. Hence, let's create a Spark session."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Create Spark instance\n\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now let's download the data from my GitHub repo into the notebook path."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Download data from GitHub as a parquet file\n\n!wget https://github.com/soundarzozm/Iot-Sensor-Data/raw/master/df.parquet?raw=true\n!mv df.parquet?raw=true df.parquet"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Since the dataset is now present in the notebook path, it is time to read the dataset using Apache Spark and create a dataframe instance of the dataset.\nLet's name the dataframe as df and the dataset as sensor_data."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Create a spark dataframe out of the parquet file\ndf = spark.read.parquet('df.parquet')\n\n#Create an instance of the dataset and name the table as sensor_data\ndf.createOrReplaceTempView('sensor_data')\n\n#Print the statistical description of the dataframe\ndf.describe().show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "From the **Data Exploration** notebook we concluded that *Device 3* is most appropriate to work on.<br>\n<br>\nHence, we create a new dataframe with only that particular device's values and perform further analysis.<br>\nLet's also print the first 20 records of the dataframe to confirm."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Create a new dataframe df_3 having data only from the third device\ndf_3 = spark.sql(\"SELECT * from sensor_data where device = 'b8:27:eb:bf:9d:51'\")\n\n#Print first 20 records of the dataframe\ndf_3.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let's define the correlation matrix function and plot the matrix."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Import necessary libraries\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.feature import VectorAssembler\n\n#Create label list\nlabels = ['co', 'humidity', 'light', 'lpg', 'motion', 'smoke', 'temp']\n\n#Define function that accepts a spark dataframe and returns Correlation Matrix \ndef correlation_matrix(dataframe, labels):\n    \n    #Create mew column called corr_features containing necessary features for the matrix\n    assembler = VectorAssembler(inputCols=labels, outputCol=\"corr_features\")\n    \n    #Call the assembler to create an instance\n    df_vector = assembler.transform(dataframe).select(\"corr_features\")\n\n    #Get correlation matrix\n    matrix = Correlation.corr(df_vector, \"corr_features\").collect()[0][0]\n    \n    #Convert to Python list fornat\n    cor_mat = matrix.toArray().tolist()\n    return cor_mat"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Plot the Correlation Matrix using Seaborn\nsns.set(rc={'figure.figsize':(11.7,8.27), \"axes.titlesize\":20})\nsns.heatmap(correlation_matrix(df_3, labels), cmap=\"Blues\", xticklabels=labels, yticklabels=labels).set_title('Correlation Matrix\\nDevice 3')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We notice that **co**, **lpg**, and **smoke** are very highly correlated to each other (as studied from the *Data Exploration Notebook*).<br>\n<br>\nHence, we can go ahead and **drop** any two of these features since it would remove redundancy from the data."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Delete the redundant columns from the dataframe \ndf_3 = df_3.drop('co').drop('lpg')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let's print the dataframe after dropping **co** and **lpg** from the dataframe."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Display the first 20 records of the dataframe\ndf_3.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "That looks great!<br>\n<br>\nNow, for the next step, we need to convert the *boolean* entries in **motion** and **light** to *integer* values (0 and 1) because *machine learning* requires data in numerical format.<br>\nWe do this by defining a function which takes a boolean argument and returns the corresponding integer value (0 for False, 1 for True), and then apply this function columnwise and store the output in a new column."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Define function to convert boolean values to integer values\ndef bool_to_int(x):\n    if x == False:\n        return 0\n    elif x == True:\n        return 1\n    else:\n        return x"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Import necessary modules\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.functions import udf\n\n#Create a user defined function to be applied on dataframe using the previously defined function\nbool_to_int_udf = udf(lambda x: bool_to_int(x), IntegerType())"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Apply the user defined function to the dataframe's light column and create a new column called light_final having integer values\ndf = df_3.withColumn(\"light_final\", bool_to_int_udf(df_3.light))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Display first 20 records of the database\ndf.show()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Apply the user defined function to the dataframe's motion column and create a new column called motion_final having integer values\ndf = df.withColumn(\"motion_final\", bool_to_int_udf(df_3.motion))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Display first 20 records of the dataframe\ndf.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now that we have the required columns, we can **drop** the old ones. Hence we drop *light* and *motion*."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Delete unnecessary columns from the dataframe\ndf = df.drop('ts').drop('device').drop('light').drop('motion')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let's print the first 20 records of the final dataframe to confirm the application of the necessary transformations."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Display first 20 records of the dataframe\ndf.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h3>Perfect!</h3>\n<br>\nWe observe that the dataframe is has numerical data throughout and the data makes complete sense.<br>\nHence, we can move ahead to feeding this data to machine learning models.<br>\n<br>\nLet's have a final analysis on the dataframe using statistical description and a correlation matrix. \n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Display statistical description of the final dataframe\ndf.describe().show()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Define new labels list\nlabels = ['humidity', 'light_final','motion_final', 'smoke', 'temp']\n\n#Plot the Correlation Matrix using Seaborn\nsns.set(rc={'figure.figsize':(11.7,8.27), \"axes.titlesize\":20})\nsns.heatmap(correlation_matrix(df, labels), cmap=\"Blues\", xticklabels=labels, yticklabels=labels).set_title('Correlation Matrix')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We observe that correlation between all the columns are very favourable.<br>\nLet's export the dataframe to a parquet file in the notebook path since we'll be using it in the next notebook which is about model definition, training and deployment."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\"\"\"project.save_data(file_name = \"df_final.parquet\",data = df.write.parquet(\"df_final.parquet\"), overwrite=True, set_project_asset = True)\"\"\""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}